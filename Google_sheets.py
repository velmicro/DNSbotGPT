import logging
from typing import List, Dict, Any
import json
import os
import faiss
import numpy as np
import gspread
from google.oauth2.service_account import Credentials
from sentence_transformers import SentenceTransformer
from Config import GOOGLE_CREDENTIALS_PATH, SPREADSHEET_ID
import re
import requests
from bs4 import BeautifulSoup


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/corrections.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ SentenceTransformer...")
model = SentenceTransformer('all-MiniLM-L6-v2')
logger.info("–ú–æ–¥–µ–ª—å SentenceTransformer —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
knowledge_base: List[Dict[str, Any]] = []
vector_index: faiss.IndexFlatL2 = None
questions: List[str] = []

# –ü—É—Ç—å –∫ —Ñ–∞–π–ª–∞–º –∫–µ—à–∞
CACHE_DIR = "cache"
KNOWLEDGE_BASE_CACHE = os.path.join(CACHE_DIR, "knowledge_base.json")
QUESTIONS_CACHE = os.path.join(CACHE_DIR, "questions.json")
INDEX_CACHE = os.path.join(CACHE_DIR, "vector_index.bin")
TIMESTAMP_CACHE = os.path.join(CACHE_DIR, "last_modified.txt")

if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–æ–±—â–∏–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω–µ—Å—É—Ç —Å–º—ã—Å–ª–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞)
STOP_WORDS = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '—É', '–∫–∞–∫', '–≤—Å–µ', '–∞', '–¥–ª—è', '—Ç–æ', '—á—Ç–æ', '—ç—Ç–æ', '–Ω–µ', '–∏–ª–∏', '–µ—Å–ª–∏'}

def init_google_sheets():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Google Sheets.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç gspread.Spreadsheet –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        creds_path = os.getenv("GOOGLE_CREDENTIALS_PATH")
        spreadsheet_id = os.getenv("SPREADSHEET_ID")
        if not creds_path or not spreadsheet_id:
            logger.error("GOOGLE_CREDENTIALS_PATH –∏–ª–∏ SPREADSHEET_ID –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤ .env")
            return None

        creds = Credentials.from_service_account_file(creds_path, scopes=[
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ])
        client = gspread.authorize(creds)
        sheet = client.open_by_key(spreadsheet_id)
        logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Google Sheets")
        return sheet
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Google Sheets: {e}")
        return None

def save_cache(knowledge_base: List[Dict[str, Any]], questions: List[str], vector_index: faiss.IndexFlatL2, last_modified: str):
    try:
        with open(KNOWLEDGE_BASE_CACHE, 'w', encoding='utf-8') as f:
            json.dump(knowledge_base, f, ensure_ascii=False, indent=4)
        with open(QUESTIONS_CACHE, 'w', encoding='utf-8') as f:
            json.dump(questions, f, ensure_ascii=False, indent=4)
        faiss.write_index(vector_index, INDEX_CACHE)
        with open(TIMESTAMP_CACHE, 'w', encoding='utf-8') as f:
            f.write(last_modified)
        logger.info("–ö–µ—à —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞: {e}")

def load_cache() -> tuple[List[Dict[str, Any]], faiss.IndexFlatL2, List[str], str]:
    try:
        if not all(os.path.exists(path) for path in [KNOWLEDGE_BASE_CACHE, QUESTIONS_CACHE, INDEX_CACHE, TIMESTAMP_CACHE]):
            logger.info("–§–∞–π–ª—ã –∫–µ—à–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
            return [], None, [], ""
        with open(KNOWLEDGE_BASE_CACHE, 'r', encoding='utf-8') as f:
            knowledge_base = json.load(f)
        with open(QUESTIONS_CACHE, 'r', encoding='utf-8') as f:
            questions = json.load(f)
        vector_index = faiss.read_index(INDEX_CACHE)
        with open(TIMESTAMP_CACHE, 'r', encoding='utf-8') as f:
            last_modified = f.read().strip()
        logger.info("–ö–µ—à —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
        return knowledge_base, vector_index, questions, last_modified
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}")
        return [], None, [], ""

def load_knowledge_base() -> tuple[List[Dict[str, Any]], faiss.IndexFlatL2, List[str]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ Google Sheets.
    """
    global knowledge_base, vector_index, questions

    sheet = init_google_sheets()
    if not sheet:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
        return [], None, []

    try:
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Google Sheets...")
        data = sheet.sheet1.get_all_records()
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
        if not data:
            logger.warning("Google Sheets –ø—É—Å—Ç")
            return [], None, []

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Keywords –≤ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        for entry in data:
            if "Keywords" in entry:
                entry["Keywords"] = str(entry["Keywords"])  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É

        knowledge_base = data
        questions = [row["Question"] for row in data if "Question" in row and row["Question"]]
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {len(questions)}")
        if not questions:
            logger.warning("–í–æ–ø—Ä–æ—Å—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
            return knowledge_base, None, []

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
        logger.info("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤...")
        question_embeddings = model.encode(questions, show_progress_bar=True)
        dimension = question_embeddings.shape[1]
        vector_index = faiss.IndexFlatL2(dimension)
        vector_index.add(np.array(question_embeddings, dtype=np.float32))
        logger.info("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–µ—à–∞
        sheet_metadata = sheet.fetch_sheet_metadata()
        last_modified = sheet_metadata.get('properties', {}).get('modifiedTime', '')
        save_cache(knowledge_base, questions, vector_index, last_modified)
        return knowledge_base, vector_index, questions
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
        raise

async def initialize_knowledge_base():
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.
    """
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
    global knowledge_base, vector_index, questions
    knowledge_base, vector_index, questions = load_knowledge_base()
    logger.info(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {len(knowledge_base)} –∑–∞–ø–∏—Å–µ–π, {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")

async def get_relevant_entries(query: str) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤.
    """
    global knowledge_base, vector_index, questions

    logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º: knowledge_base={len(knowledge_base)} –∑–∞–ø–∏—Å–µ–π, questions={len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤, vector_index={'–∑–∞–≥—Ä—É–∂–µ–Ω' if vector_index is not None else '–Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω'}")

    if not knowledge_base or vector_index is None or not questions:
        logger.warning("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞...")
        loaded_knowledge_base, loaded_vector_index, loaded_questions = load_knowledge_base()
        if not loaded_knowledge_base or loaded_vector_index is None or not loaded_questions:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
            return "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ! üòî"
        knowledge_base, vector_index, questions = loaded_knowledge_base, loaded_vector_index, loaded_questions
        logger.info(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(knowledge_base)} –∑–∞–ø–∏—Å–µ–π, {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")

    try:
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_lower = re.sub(r'[^\w\s]', '', query.lower()).strip()
        query_words = set(query_lower.split()) - STOP_WORDS  # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        logger.info(f"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {query_lower}, —Å–ª–æ–≤–∞: {query_words}")

        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –≤–æ–ø—Ä–æ—Å–∞–º –∏ –æ—Ç–≤–µ—Ç–∞–º
        relevant_entries = []
        for idx, entry in enumerate(knowledge_base):
            question = entry.get("Question", "–í–æ–ø—Ä–æ—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            answer = entry.get("Answer", "–û—Ç–≤–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Keywords
            keywords_raw = entry.get("Keywords", "")
            if not isinstance(keywords_raw, str):
                logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Keywords –≤ –∑–∞–ø–∏—Å–∏ {idx}: {keywords_raw} (—Ç–∏–ø: {type(keywords_raw)}). –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É.")
                keywords_raw = str(keywords_raw)
            keywords = keywords_raw.lower().split(",")
            keywords = [kw.strip() for kw in keywords if kw.strip()]
            keywords_set = set(keywords) - STOP_WORDS

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Question
            question_lower = re.sub(r'[^\w\s]', '', question.lower()).strip()
            question_words = set(question_lower.split()) - STOP_WORDS

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Answer
            answer_lower = re.sub(r'[^\w\s]', '', answer.lower()).strip()
            answer_words = set(answer_lower.split()) - STOP_WORDS

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            matched_keywords = [kw for kw in keywords_set if kw in query_words]
            matched_question_words = [word for word in question_words if word in query_words]
            matched_answer_words = [word for word in answer_words if word in query_words]

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            # –î–∞—ë–º —Ä–∞–∑–Ω—ã–π –≤–µ—Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º: Keywords - 1.0, Question - 0.8, Answer - 0.6
            keyword_score = len(matched_keywords) / (len(keywords_set) if keywords_set else 1) * 1.0
            question_score = len(matched_question_words) / (len(question_words) if question_words else 1) * 0.8
            answer_score = len(matched_answer_words) / (len(answer_words) if answer_words else 1) * 0.6
            total_score = keyword_score + question_score + answer_score

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            RELEVANCE_THRESHOLD = 0.5
            if total_score < RELEVANCE_THRESHOLD:
                continue

            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å
            if matched_keywords or matched_question_words or matched_answer_words:
                # –û–±—Ä–µ–∑–∞–µ–º –æ—Ç–≤–µ—Ç –¥–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤
                if len(answer) > 1000:
                    answer = answer[:1000] + "..."
                relevant_entries.append({
                    "question": question,
                    "answer": answer,
                    "matched_keywords": matched_keywords,
                    "matched_question_words": matched_question_words,
                    "matched_answer_words": matched_answer_words,
                    "score": total_score
                })
                logger.info(
                    f"–ù–∞–π–¥–µ–Ω–∞ –∑–∞–ø–∏—Å—å: –í–æ–ø—Ä–æ—Å: {question}, "
                    f"–°–æ–≤–ø–∞–≤—à–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {matched_keywords}, "
                    f"–°–æ–≤–ø–∞–≤—à–∏–µ —Å–ª–æ–≤–∞ –≤ –≤–æ–ø—Ä–æ—Å–µ: {matched_question_words}, "
                    f"–°–æ–≤–ø–∞–≤—à–∏–µ —Å–ª–æ–≤–∞ –≤ –æ—Ç–≤–µ—Ç–µ: {matched_answer_words}, "
                    f"–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {total_score:.2f}"
                )

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevant_entries = sorted(relevant_entries, key=lambda x: x["score"], reverse=True)

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é
        if not relevant_entries:
            logger.info("–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Å–ª–æ–≤–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
            query_embedding = model.encode([query])[0]
            query_embedding = np.array([query_embedding], dtype=np.float32)

            # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –∑–∞–ø–∏—Å–µ–π (—Ç–æ–ø-3)
            distances, indices = vector_index.search(query_embedding, 3)

            VECTOR_RELEVANCE_THRESHOLD = 0.1  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            for idx, distance in zip(indices[0], distances[0]):
                if idx >= len(knowledge_base):
                    logger.warning(f"–ò–Ω–¥–µ–∫—Å {idx} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ knowledge_base (–¥–ª–∏–Ω–∞: {len(knowledge_base)})")
                    continue
                entry = knowledge_base[idx]
                question = entry.get("Question", "–í–æ–ø—Ä–æ—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                answer = entry.get("Answer", "–û—Ç–≤–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                keywords_raw = entry.get("Keywords", "")
                if not isinstance(keywords_raw, str):
                    logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Keywords –≤ –∑–∞–ø–∏—Å–∏ {idx} (–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è): {keywords_raw} (—Ç–∏–ø: {type(keywords_raw)}). –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É.")
                    keywords_raw = str(keywords_raw)
                keywords = keywords_raw.lower()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é
                if distance > VECTOR_RELEVANCE_THRESHOLD:
                    logger.info(f"–ó–∞–ø–∏—Å—å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ (–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è): –í–æ–ø—Ä–æ—Å: {question}, –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance}")
                    continue

                # –û–±—Ä–µ–∑–∞–µ–º –æ—Ç–≤–µ—Ç –¥–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤
                if len(answer) > 1000:
                    answer = answer[:1000] + "..."
                
                relevant_entries.append({
                    "question": question,
                    "answer": answer,
                    "matched_keywords": [],
                    "matched_question_words": [],
                    "matched_answer_words": [],
                    "score": 1 - distance  # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
                })
                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –∑–∞–ø–∏—Å—å –ø–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: –í–æ–ø—Ä–æ—Å: {question}, –û—Ç–≤–µ—Ç: {answer}, –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if not relevant_entries:
            logger.warning("–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return "–ù–µ –Ω–∞—à—ë–ª –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å! üòÖ"

        # –ë–µ—Ä—ë–º —Ç–æ–ø-3 –∑–∞–ø–∏—Å–∏ (–∏–ª–∏ –º–µ–Ω—å—à–µ, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –º–µ–Ω—å—à–µ)
        result = ""
        for i, entry in enumerate(relevant_entries[:3]):
            result += f"–ó–∞–ø–∏—Å—å {i+1}:\n–í–æ–ø—Ä–æ—Å: {entry['question']}\n–û—Ç–≤–µ—Ç: {entry['answer']}\n\n"
        logger.info(f"–ü–µ—Ä–µ–¥–∞—ë–º –≤ Groq —Å–ª–µ–¥—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n{result}")
        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {e}")
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ! üòî"

def add_to_knowledge_base(question: str, keywords: str, answer: str):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –≤ Google Sheets.
    """
    try:
        sheet = init_google_sheets()
        if not sheet:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Google Sheets –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏")
            return
        
        sheet.sheet1.append_row([question, keywords, answer])
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∑–∞–ø–∏—Å—å: –í–æ–ø—Ä–æ—Å: {question}, –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {e}")
        raise

    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ Google Sheets
        sheet.sheet1.append_row([question, keywords, answer])
        logger.info(f"–ù–æ–≤–∞—è –∑–∞–ø–∏—Å—å –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ Google Sheets: {question}")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏ –∫–µ—à
        global knowledge_base, vector_index, questions
        knowledge_base.append({"Question": question, "Keywords": keywords, "Answer": answer})
        questions.append(question)

        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        new_embedding = model.encode([question])[0]
        vector_index.add(np.array([new_embedding], dtype=np.float32))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –∫–µ—à
        sheet_metadata = sheet.fetch_sheet_metadata()
        last_modified = sheet_metadata.get('properties', {}).get('modifiedTime', '')
        save_cache(knowledge_base, questions, vector_index, last_modified)

        logger.info(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞: –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–ø–∏—Å—å '{question}'")
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∑–∞–ø–∏—Å–∏ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {e}")
        return False

async def parse_and_add_to_sheet(url: str) -> bool:
    """
    –ü–∞—Ä—Å–∏—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π —Å–∞–π—Ç, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –æ—Ç–≤–µ—Ç—ã, –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏—Ö –≤ Google Sheets.
    """
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (–≤–æ–ø—Ä–æ—Å—ã) –∏ —Ç–µ–∫—Å—Ç (–æ—Ç–≤–µ—Ç—ã)
        entries = []
        for header in soup.find_all(['h1', 'h2', 'h3']):
            question = header.get_text(strip=True)
            if not question.endswith('?'):
                question += '?'
            answer = ""
            next_element = header.find_next_sibling()
            while next_element and next_element.name not in ['h1', 'h2', 'h3']:
                if next_element.name == 'p':
                    answer += next_element.get_text(strip=True) + "\n"
                next_element = next_element.find_next_sibling()
            answer = answer.strip()
            if not answer:
                continue
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
            keywords = ",".join(set(re.sub(r'[^\w\s]', '', question.lower()).split()) - STOP_WORDS)[:3]
            entries.append({"question": question, "keywords": keywords, "answer": answer})

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –≤ Google Sheets
        for entry in entries:
            success = add_to_knowledge_base(entry["question"], entry["keywords"], entry["answer"])
            if not success:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å—å: {entry['question']}")
                return False

        logger.info(f"–£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(entries)} –∑–∞–ø–∏—Å–µ–π –∏–∑ —Å–∞–π—Ç–∞ {url}")
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å–∞–π—Ç–∞ {url}: {e}")
        return False